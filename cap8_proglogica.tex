\section{Progettazione Logica}

\subsection{Modelli logici per il data mart}
Prendere gli schemi di fatto e tradurli in una implementazione coerente con il modello scelto (ROLAP o MOLAP). Esistono due tipi di schemi standard che si usano quando si implementa un cubo su una piattaforma ROLAP. Il principale è lo schema a stella ed è un modo classico per rappresentare un contenuto multidimensionale su un DB relazionale. Uno schema a stella è composto da:
\begin{itemize}
	\item 
	\textbf{Dimension Table}: una per ogni dimensione del cubo, in cui si ha una chiave primaria (un surrogato) e dentro si hanno tutti gli attributi della gerarchia
	\item
	\textbf{Fact table}: importa al suo interno Foreign Key prese da tutte le DT. Tutte le FK insieme formano la chiave primaria e in più si trovano le misure
\end{itemize}

\textbf{Considerazioni}

Le DT sono completamente denormalizzate, non sono in terza forma normale e non si hanno problemi di sparsità in quanto vengono memorizzzate soltanto le tuple corrispondenti a punti dello spazio multidimensionale per cui esistono eventi. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/stella}
	\caption{Schema a Stella}
	\label{fig:stella}
\end{figure}
Il secondo tipo di schema che si usa si chiama snowflake, il quale è una variante dello schema a stella, la cui caratteristica è che le DT vengono completamente o parzialmente normalizzate. 

\textbf{Considerazioni}

\begin{itemize}
	\item
	Lo spazio richiesto si riduce in linea di principio perché normalizzando elimino le ridondanze, ma è anche vero che ho aggiunto dei surrogati che nello schema a stella non si hanno
	\item 
	L’esecuzione di interrogazioni che coinvolgono solo gli attributi contenuti nella fact table e nelle dimension table primarie è avvantaggiata
	\item 
	Il tempo di esecuzione delle interrogazioni che coinvolgono attributi delle dimension table secondarie aumenta
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/snowflake}
	\caption{Snowflake}
	\label{fig:snowflake}
\end{figure}

\subsection{Le viste}
La tecnica più usata in ambito ROLAP per ottimizzare le prestazioni è la cosiddetta materializzazione delle viste, in quando ci saranno query frequentemente richieste, allora per ottimizzare queste query materializzo, cioè creo fisicamente nel DB delle viste (ossia tabelle) che memorizzano esattamente il risultato della query. 

Nelle viste normalmente non si mettono dei predicati di selezioni, ma tutti gli eventi aggregati in un certo modo. La vista viene denotata in base ad un certo group-by set. 

L’utilità nel creare una vista, non è solo limitata alle query che hanno quel group-by ma anche a query che sono ulteriormente aggregate. Questo ragionamento viene supportato dal reticolo, il quale è una struttura algebrica.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/reticolo}
	\caption{Reticolo multidimensionale}
	\label{fig:reticolo}
\end{figure}

Supponiamo di avere il DFM in figura \ref{fig:reticolo} con due dimensioni e due micro-gerarchie. A destra troviamo il reticolo multidimensionale completo, cioè si ha dentro un nodo per ogni possibile group-by di quel DFM. Sono tutti disposti in questa struttura, in cui si ha sempre un top e un bottom. Il primo è quello più fine, quello che corrisponde alle dimensioni. Il bottom è il group-by vuoto, aggrego tutto in un mega evento secondario. Le frecce indicano che da un dato aggregato facendo un ulteriore aggregazione posso calcolare un dato che è ancora più aggregato. Se scelgo la vista più in alto possibile, sto dicendo che questa la uso spesso, in quanto mi copre più query, ma dovrei aggregare sempre. Avendola scelta così in alto, la vista è grande, cioè quanto la fact table primaria e mi porta ad un incremento delle prestazioni trascurabile. Se la scegliessi in basso migliora ma per poche query. In realtà la materializzazione delle viste deve essere fatta sul carico di lavoro. 
\subsubsection{Schemi relazioniali e viste}
Adottando lo snowflake schema è possibile memorizzare in fact table separate dati appartenenti a diversi group-by set.


Nei motori multidimensionali vi è un componente, l’aggregate navigator, che in maniera trasparente per l’utente decide quale vista utilizzare per ciascuna query. Ha accesso ai metadati, sa come sono fatte le gerarchie, conosce le dipendenze funzionali, sa quali sono le viste che gli utenti hanno creato. Ha il compito di capire su quale fact table quella query è eseguibile.

\subsection{Progettazione logica}
Include l’insieme dei passi che, a partire dallo schema concettuale, permettono di determinare lo schema logico del data mart. In input ho lo schema concettuale, conosco il carico di lavoro e il volume dati, e in output, devo ottenere lo schema logico. I principi basati per fare questa traduzione non sono gli stessi utilizzati per i DB operazionali. Gli step da fare sono:
\begin{enumerate}
	\item
	Scelta dello schema logico da utilizzare (schema a stella o snowflake) 
	\item 
	Traduzione degli schemi concettuali
	\item 
	Scelte delle viste da materializzare
	\item 
	Applicazione di altre forme di ottimizzazione
\end{enumerate}

\textbf{Star vs Snowflake}

Primo ragionamento che si può fare è legato alla cardinalità di dominio, cioè al numero di valori distinti per ogni attributo. L’unico caso in cui si utilizza snowflake è quando si ha una gerarchia condivisa. 

La regola di base per la traduzione di uno schema di fatto in schema a stella prevede di: 
\begin{itemize}
	\item
	Creare una fact table contenente tutte le misure e gli attributi descrittivi direttamente collegati con il fatto
	\item 
	Per ogni gerarchia, creare una dimension table che ne contiene tutti gli attributi.
\end{itemize}
In aggiunta a questa semplice regola, la corretta traduzione di uno schema di fatto richiede una trattazione approfondita dei costrutti avanzati del DFM:
\begin{itemize}
	\item 
	\textbf{Attributi descrittivi}: finisce nella stessa DT nella quale finisce suo padre
	\item
	\textbf{Archi opzionali}: tutti gli attributi a valle dell’arco opzionale finiscono nella stessa DT del padre.
	\item 
	\textbf{Gerarchia condivisa}: se è su una dimensione, allora si crea un unico DT e la si usa più volte. Se invece la condivisione è da qualche altra parte e non sulla dimensione allora ho due scelte: snowflaking o ripetere gi attributi in entrambe le tabelle. 
	\item 
	\textbf{Convergenza}: si includono nella stessa DT dei loro attributi padri
	\item 
	\textbf{Gerarchie incomplete}: basta sapere che dal punto di vista dello schema non ci sono impatti particolari
	\item 
	\textbf{Scelta delle viste}: è un compito complesso, la soluzione rappresenta un trade-off tra numerosi requisiti in contrasto: 
	\begin{enumerate}
		\item 
		Minimizzazione di funzioni di costo
		\item 
		Vincoli di sistema
		\begin{itemize}
			\item 
			Spazio su disco
			\item 
			Tempo a disposizione per l’aggiornamento dei dati (ETL)
		\end{itemize}
		\item 
		Vincoli utente
		\begin{itemize}
			\item
			Tempo massimo di risposta
			\item 
			Freschezza dei dati
		\end{itemize}
	\end{enumerate}
	Per prima cosa si cerca di determinare le viste candidate, ossia quelle utili a ridurre il costo di esecuzione del lavoro. Nella pratica si usa un approccio-greedy. 
	
	È utile materializzare una vista quando:
	\begin{itemize}
		\item
		Risolve direttamente un’interrogazione frequente
		\item 
		Permette di ridurre il costo di esecuzione di molte interrogazioni
	\end{itemize}
	Non è consigliabile materializzare una vista quando:
	\begin{itemize}
		\item
		Il suo group-by set è molto simile a quello di una vista già
		materializzata
		\item
		Il suo group-by set è molto fine
		\item 
		La materializzazione non riduce di almeno un ordine di grandezza il costo delle interrogazioni
	\end{itemize}
	
\end{itemize}

\subsection{Progettazione dell'ETL}
L’obiettivo di questa fase è capire come estrarre i dati dal DB sorgente, trasformarli e pulirli per poi caricarli nel data mart. È anche la fase più costosa perché bisogna scrivere del software. Nel nostro caso facciamo riferimento ad un’architettura a tre livelli. Dunque, in presenza dell’ODS l’ETL si compone di due step:
\begin{itemize}
	\item 
	Dalle sorgenti all’ODS (database riconciliato)
	\item 
	Dall’ODS al data mart
\end{itemize}
Le fasi dell’ETL sono estrazione, trasformazione, pulizia e caricamento. Si ricorda che la differenza tra trasformazione e pulizia è che la prima si occupa del formato dei dati, mentre l'altra si occupa di aumentare la qualità del dato. Vi è un DB su cui ruotano le varie fasi che si chiama \textbf{staging area}, ovvero uno spazio utilizzato per memorizzare in via transitoria le informazioni necessarie all’esecuzione delle procedure.

Esistono due modi per fare \textbf{estrazione}:
\begin{itemize}
	\item 
	\textbf{Statico}: prendo tutto quello che trovo nel DB sorgente
	\item 
	\textbf{Incrementale}: estraggo solo i dati che sono stati aggiunti o modificati nelle sorgenti dall’ultima volta che ho fatto estrazione. L’estrazione incrementale può essere immediata o ritardata. La prima significa che appena la modifica del dato viene registrata nel DB sorgente, in quel momento stesso, il dato viene notificato nella staging area. Nel secondo caso, invece è l’ETL che va a capire che cosa c’è da estrarre. 
\end{itemize}
Visto che l’estrazione ha a che fare con il tempo, inevitabilmente, bisogna fare i conti con il modo in cui nel DB sorgente questa componente viene gestita. Da questo punto di vista i DB possono essere di tre tipi:
\begin{itemize}
 \item 
 \textbf{Transitori}: registro un dato e quando il dato viene modificato perdo la versione precedente e la sostituisco con quella nuova
 \item 
 \textbf{Storicizzate}: sono state progettate in modo da mantenere tutta la storia di tute le modifiche che sono state fatte
 \item 
 \textbf{Semi-storicizzata}: progettate in modo che mantengo una storia limitata nel numero di versioni
\end{itemize}
In figura \ref{fig:estrazione} è rappresentato lo schema con cui si ha a che fare, in cui troviamo il sistema operazionale, ovvero il software con la quale l’utente si interfaccia quando deve svolgere un’operazione. In mezzo, tra il sistema operazionale e i DB operazionale, troviamo il DBMS. L’utente parla con una logica applicativa, attraverso delle API, ovvero uno strato di software che fa da ponte tra la logica applicativa e il DBMS vero e proprio. Ci sono diversi modi per fare estrazione incrementale:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{img/estrazione}
	\caption{Estrazione incrementale}
	\label{fig:estrazione}
\end{figure}
\begin{itemize}
	\item 
	\textbf{Estrazione assistita dall’applicazione}: significa che in linea di principio dovrei mettere le mani sul sistema operazionale e aggiungere del codice, per fare in modo, che tutte le volte che in questo codice viene dato un comando per aggiornare un dato, venga anche lanciata una procedura che scrive sulla staging area. Il problema è che cambiare il codice per scrivere tutte le procedure ha un costo a meno che non vi sia uno strato di API, perché ho delle funzioni standard che gestiscono tutte le operazioni di inserimento, modifica e cancellazione, e dunque la procedura aggiuntiva la metto solo li e non nella logica applicativa. In pratica in molti casi non è utilizzabile. 
	\item 
	\textbf{Estrazione basata su log}: quando lancio l’ETL, questo si prende il log, isola la porzione di log che è stata scritta nell’ultima settimana e va a ricostruire tutto quello che il DBMS ha fatto. Il problema è che l’interpretazione del log è complicatissima. Per fortuna in alcuni il DBMS da dei componenti per interpretare il log.
	\item 
	\textbf{Estrazione basata su trigger}: sono delle procedure che si possono scrivere sul DBMS abbinato su un DB. Ad esempio, alla tabella studenti, abbino un trigger che esegue una procedura ogni qual volta un record viene modificato. Dentro la procedura scrivo il codice che notifica la variazione alla staging area. Il bello è che i trigger li gestisce il DBMS, non interferendo con la logica applicativa. È una tecnica immediata, ma per riuscire a cogliere tutte le modifiche che faccio sul DB di trigger ne servono centinaia. Il problema vero a questo punto è di prestazione, perché il DBMS a ogni singola operazione deve controllare se può lanciare qualcuno di questi trigger.
	\item 
	\textbf{Estrazione basata su marche temporali}: un attributo i cui valori sono del tipo data, ora, minuto, secondo, millisecondo. L’idea è che se nel DB in ogni tabella ho già un attributo che mi dice quando quella riga di quella tabella è stata modificata l’ultima volta fare estrazione incrementale diventa molto semplice. Con questa tecnica potrei perdermi delle modifiche, perché se il DB sorgente è storicizzato, estraggo tutto quello che mi serve. Ma se il DB o anche la singola tabella è transitorio o semi-storicizzati si possono perdere delle modifiche. 
\end{itemize}

Qualunque sia la tecnica di estrazione incrementale, il risultato che si scrive nella staging area sono tutti i dati modificati, aggiunti o cancellati rispetto allo scorso ETL. Creo una nuova tabella in cui registro tutti i record alterati scrivendo accanto che tipo di operazione è stata fatta. \\
\newline
\textbf{Caricamento dei dati}

Quello che si fa in termini di caricamento riflette quello che si è fatto nell’estrazione dei dati. Se ho fatto un’estrazione statica normalmente si fa una riscrittura completa. Se invece ho fatto un’estrazione incrementale, devo aggiungere le variazioni all’ODS, tenendo conto del tipo di operazione che ho eseguito. Devo anche considerare che livello di storicizzazione uso nell’ODS. \\
\newline
\textbf{Trasformazione e pulizia}

Ci sono diversi motivi per cui sono obbligato a pulire o a trasformare il dato. È chiaro che se si potesse avere una prevenzione degli errori, con controlli dell’input molto stretti, la maggior parte degli errori non ci sarebbero. Si possono utilizzare diverse tecniche:
\begin{itemize}
	\item 
	\textbf{Basate su dizionari}: si utilizzano quando bisogna standardizzare o pulire dati che hanno un riferimento noto
	\item 
	\textbf{Business rules}: sono regole esprimibili attraverso equazioni che devono essere sempre verificate dai dati che caricano  
	\item 
	Fusione approssimata: riuscire a stabilire delle corrispondenze tra righe della stessa tabella in assenza di chiavi. Questa tecnica ha due sfaccettature: join approssimati e purge/merge
\end{itemize}
Come misuro la similarità tra due record? Per esempio, ho due indirizzi o due cognomi, dunque in entrambi i casi ho due stringhe da confrontare. Quello che si fa, di solito, per ogni singola parola di ciascuna stringa è trovare il match migliore con l’altra stringa. Dopodiché uso la edit distance, ovvero il numero di caratteri che devo modificare nell’una per ottenere l’altra. 

Altro problema sono le abbreviazioni, perché riuscire a trovare tutte queste possibilità è relativamente complesso. Infine, posso scrivermi delle regole per euristicamente stimare la similarità tra due record. 

A questo punto siamo riusciti a caricare i dati nell’ODS e quindi adesso bisogna aggiornare il data mart, aggiornando DT, FT e viste. Prendo le righe modificate dallo scorso ETL, faccio i join che mi servono e preparo la tupla quasi pronta per la DT. Resta solo un problema, l’ID perché l’ID del prodotto che ho li non è lo stesso ID presente nel data mart, perché nel data mart uso le chiavi surrogate. Quindi devo fare una sostituzione di chiavi attraverso tabelle di look-up dove si mantengono le corrispondenze tra le due chiavi. Adesso ho pronto la nuova riga pronta per il caricamento nel DT. Sono pronto per aggiornare le FT. Anche qui nel DB sorgente ho le nuove righe, estraggo le righe modificate dall’ETL, faccio sempre i join, group-by, e ho la tupla pronta per essere inserita nella FT, salvo per le chiavi, in cui uso le tabelle di look-up. 

Per rinfrescare le viste materializzate rilancerò tutte le query.


